---
title: "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection"
collection: publications
category: conferences
permalink: /publication/2025-05-21-Practical-1
excerpt: 'Adversarial attacks on stochastic bandits often rely on unrealistic assumptions, such as unbounded perturbations or per-round reward manipulation, limiting their practical applicability. This work introduces Fake Data Injection, a novel threat model where adversaries inject a limited number of bounded fake feedback samples into the learner’s history, mimicking legitimate interactions. We design efficient attack strategies that respect real-world constraints—bounded reward magnitudes and restricted injection frequency—and theoretically demonstrate their ability to deceive both UCB and Thompson Sampling algorithms into persistently selecting a target arm with only sublinear attack cost. Empirical results on synthetic and real-world datasets confirm the vulnerabilities of widely used bandit algorithms under this practical adversarial scenario.'
date: 2025-05-21
venue: 'arXiv'
# slidesurl: 'http://qirunzeng.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/pdf/2504.15812'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems.
We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions.
We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected).
Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.